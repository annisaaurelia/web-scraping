{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import selenium\n",
    "import time\n",
    "import sys\n",
    "import linecache\n",
    "import selenium.webdriver as webdriver\n",
    "import selenium.webdriver.support.ui as ui\n",
    "import pymongo\n",
    "import datetime\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium import webdriver\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep \n",
    "from pymongo import MongoClient \n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAZADA Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print exception\n",
    "def PrintException():\n",
    "    exc_type, exc_obj, tb = sys.exc_info()\n",
    "    f = tb.tb_frame\n",
    "    lineno = tb.tb_lineno\n",
    "    filename = f.f_code.co_filename\n",
    "    linecache.checkcache(filename)\n",
    "    line = linecache.getline(filename, lineno, f.f_globals)\n",
    "    print ('EXCEPTION IN ({}, LINE {} \"{}\"): {}'.format(filename, lineno, line.strip(), exc_obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 400 per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scraping the page\n",
    "def scrape_detail_page(driver,category,collection):\n",
    "    time.sleep(5)\n",
    "    # scraping page\n",
    "    path = driver.find_elements_by_class_name('c2prKC')\n",
    "    check = driver.find_elements_by_class_name('c1DXz4')[0].text\n",
    "    while(check is None):\n",
    "        path = driver.find_elements_by_class_name('c2prKC')\n",
    "        time.sleep(2)\n",
    "    result = pd.DataFrame()\n",
    "    # print(len(path))\n",
    "    for x in range(len(path)):\n",
    "        error = 0\n",
    "        while(error<5):\n",
    "            try:\n",
    "                location = path[x].location['y']-100\n",
    "                driver.execute_script(\"window.scrollTo(0, {});\".format(location))\n",
    "                try:\n",
    "                    origin_price = re.sub(\"\\.\",\"\",re.sub(\"Rp\",\"\",path[x].find_element_by_css_selector('.c3lr34').find_element_by_css_selector('.c13VH6').text))\n",
    "                except:\n",
    "                    origin_price = re.sub(\"\\.\",\"\",re.sub(\"Rp\",\"\",path[x].find_element_by_css_selector('.c3gUW0').find_element_by_css_selector('.c13VH6').text))\n",
    "\n",
    "                try:\n",
    "                    discounted_price = re.sub(\"\\.\",\"\",re.sub(\"Rp\",\"\",path[x].find_element_by_css_selector('.c3gUW0').find_element_by_css_selector('.c13VH6').text))\n",
    "                except:\n",
    "                    discounted_price = origin_price\n",
    "\n",
    "                image = path[x].find_element_by_xpath('div/div/div[1]/div[1]/a/img').get_attribute('src')\n",
    "                sku = path[x].get_attribute('data-item-id')\n",
    "                name = driver.find_elements_by_class_name('c16H9d')[x].text\n",
    "\n",
    "                while(image is None):\n",
    "                    location = path[x].location['y']-100\n",
    "                    driver.execute_script(\"window.scrollTo(0, {});\".format(location))\n",
    "                    image = path[x].find_element_by_xpath('div/div/div[1]/div[1]/a/img').get_attribute('src')\n",
    "                    time.sleep(1)\n",
    "\n",
    "                # Return result\n",
    "                date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "                new_result = pd.DataFrame([{'sku':sku,'name':name, 'origin_price':origin_price, 'discounted_price':discounted_price, 'image':image,'category':category,'scraping_date':date}])\n",
    "                result = result.append(new_result)\n",
    "                #print(new_result)\n",
    "                \n",
    "                break\n",
    "                \n",
    "            except:\n",
    "                PrintException()\n",
    "                error=error+1\n",
    "            \n",
    "    # save to mongodb\n",
    "    try:\n",
    "        collection.insert_many(result.to_dict('records'))\n",
    "    except:\n",
    "        PrintException()\n",
    "        pass\n",
    "\n",
    "    #driver.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_url2(url,category,collection):\n",
    "        \n",
    "    # Connect to mongodb\n",
    "    try: \n",
    "        conn = MongoClient() \n",
    "        print(\"Connected successfully!!!\") \n",
    "    except:   \n",
    "        print(\"Could not connect to MongoDB\") \n",
    "    # database\n",
    "    db = conn.lazada\n",
    "    # Created or Switched to collection names: pakaian_wanita \n",
    "    collection = db[collection]\n",
    "    \n",
    "    # Load WebDriver and navigate to the page url.\n",
    "    # This will open a browser window.\n",
    "    url1 = url + \"?page=1\"\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url1)\n",
    "\n",
    "    \n",
    "    # Start Scraping\n",
    "    result = pd.DataFrame()\n",
    "    page = 1\n",
    "    start = 0\n",
    "    image_unique=0\n",
    "    while(image_unique<400):\n",
    "        # Go to catalogue page\n",
    "        result = result.append(scrape_detail_page(driver,category,collection))\n",
    "        result = result.drop_duplicates(['image'])\n",
    "        image_unique=len(result)\n",
    "\n",
    "        # Go to the next page\n",
    "        try:\n",
    "            # check_next = driver.find_elements_by_xpath('/html/body/div[3]/div/div[2]/div[1]/div/div[1]/div[3]/div/ul/li[9]/a')[0]\n",
    "            check_next = driver.find_element_by_css_selector('.ant-pagination-next').find_element_by_xpath('a')\n",
    "            try:\n",
    "                check_next.click()\n",
    "            except:\n",
    "                time.sleep(10)\n",
    "                PrintException()\n",
    "                driver.refresh()\n",
    "                #en =  driver.find_element_by_css_selector(\".nc_bg\")\n",
    "                #move = ActionChains(driver)\n",
    "                #move.click_and_hold(en).move_by_offset(100, 0).release().perform()\n",
    "                #time.sleep(5)\n",
    "            page = page+1\n",
    "            next_page = url + \"?page=\" + str(page)\n",
    "            print(\"Go to next page: \" + next_page)\n",
    "            #driver.close()\n",
    "            #driver = webdriver.Firefox()\n",
    "            #driver.get(next_page)\n",
    "        except:\n",
    "            PrintException()\n",
    "            break\n",
    "    \n",
    "    #driver.close()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pakaian_wanita > dress\n",
      "Connected successfully!!!\n",
      "Go to next page: https://www.lazada.co.id/gaun-wanita/?page=2\n",
      "EXCEPTION IN (<ipython-input-3-c28bbe36bb3f>, LINE 28 \"image = path[x].find_element_by_xpath('div/div/div[1]/div[1]/a/img').get_attribute('src')\"): Message: Unable to locate element: div/div/div[1]/div[1]/a/img\n",
      "\n",
      "EXCEPTION IN (<ipython-input-15-76eb3453a701>, LINE 37 \"check_next.click()\"): Message: Element <a class=\"ant-pagination-item-link\"> is not clickable at point (1260,581) because another element <div class=\"sufei-dialog-mask\"> obscures it\n",
      "\n",
      "Go to next page: https://www.lazada.co.id/gaun-wanita/?page=3\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ca639ee0d9d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mcollection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'collection'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_top\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_url2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcollection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-76eb3453a701>\u001b[0m in \u001b[0;36mscrape_url2\u001b[1;34m(url, category, collection)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_unique\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Go to catalogue page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscrape_detail_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcollection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mimage_unique\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-c28bbe36bb3f>\u001b[0m in \u001b[0;36mscrape_detail_page\u001b[1;34m(driver, category, collection)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# scraping page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_elements_by_class_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'c2prKC'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_elements_by_class_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'c1DXz4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_elements_by_class_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'c2prKC'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "lazada_category = pd.read_csv('C:/Users/tamu/SharePoint/Product Development - Documents/Merchandising Analytics/Analysis - QBR Jul19/Data/scraping/lazada_category.csv',sep=',')\n",
    "result = pd.DataFrame()\n",
    "for x in range(0,1):\n",
    "    category = lazada_category.iloc[x]\n",
    "    print(category['category'])\n",
    "    \n",
    "    url = category['url']\n",
    "    collection = category['collection']+\"_top\"\n",
    "    \n",
    "    result = scrape_url2(url,category['category'],collection)\n",
    "    print(result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c8b30d4b9ccb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0men\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_css_selector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".nc_iconfont\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "en =  driver.find_element_by_css_selector(\".nc_iconfont\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scraping the page\n",
    "def scrape_detail_page2(driver,category,collection,brand):\n",
    "    time.sleep(5)\n",
    "    # scraping page\n",
    "    path = driver.find_elements_by_class_name('c2prKC')\n",
    "    check = driver.find_elements_by_class_name('c1DXz4')[0].text\n",
    "    while(check is None):\n",
    "        path = driver.find_elements_by_class_name('c2prKC')\n",
    "        time.sleep(2)\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    for x in range(len(path)):\n",
    "        error = 0\n",
    "        while(error<5):\n",
    "            try:\n",
    "                location = path[x].location['y']-100\n",
    "                driver.execute_script(\"window.scrollTo(0, {});\".format(location))\n",
    "                try:\n",
    "                    origin_price = re.sub(\"\\.\",\"\",re.sub(\"Rp\",\"\",path[x].find_element_by_css_selector('.c3lr34').find_element_by_css_selector('.c13VH6').text))\n",
    "                except:\n",
    "                    origin_price = re.sub(\"\\.\",\"\",re.sub(\"Rp\",\"\",path[x].find_element_by_css_selector('.c3gUW0').find_element_by_css_selector('.c13VH6').text))\n",
    "\n",
    "                try:\n",
    "                    discounted_price = re.sub(\"\\.\",\"\",re.sub(\"Rp\",\"\",path[x].find_element_by_css_selector('.c3gUW0').find_element_by_css_selector('.c13VH6').text))\n",
    "                except:\n",
    "                    discounted_price = origin_price\n",
    "\n",
    "                image = path[x].find_element_by_xpath('div/div/div[1]/div[1]/a/img').get_attribute('src')\n",
    "                sku = path[x].get_attribute('data-item-id')\n",
    "                name = driver.find_elements_by_class_name('c16H9d')[x].text\n",
    "\n",
    "                while(image is None):\n",
    "                    location = path[x].location['y']-100\n",
    "                    driver.execute_script(\"window.scrollTo(0, {});\".format(location))\n",
    "                    image = path[x].find_element_by_xpath('div/div/div[1]/div[1]/a/img').get_attribute('src')\n",
    "                    time.sleep(1)\n",
    "\n",
    "                # Return result\n",
    "                date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "                new_result = pd.DataFrame([{'brand':brand,'sku':sku,'name':name, 'origin_price':origin_price, 'discounted_price':discounted_price, 'image':image,'category':category,'scraping_date':date}])\n",
    "                result = result.append(new_result)\n",
    "\n",
    "                \n",
    "                break\n",
    "                \n",
    "            except:\n",
    "                PrintException()\n",
    "                error=error+1\n",
    "            \n",
    "    # save to mongodb\n",
    "    try:\n",
    "        collection.insert_many(result.to_dict('records'))\n",
    "    except:\n",
    "        PrintException()\n",
    "        pass\n",
    "\n",
    "    #driver.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_url3(url,category,collection,brand):\n",
    "        \n",
    "    # Connect to mongodb\n",
    "    try: \n",
    "        conn = MongoClient() \n",
    "        print(\"Connected successfully!!!\") \n",
    "    except:   \n",
    "        print(\"Could not connect to MongoDB\") \n",
    "    # database\n",
    "    db = conn.lazada_brand\n",
    "    # Created or Switched to collection names: pakaian_wanita \n",
    "    collection = db[collection]\n",
    "    \n",
    "    # Load WebDriver and navigate to the page url.\n",
    "    # This will open a browser window.\n",
    "    url1 = url + \"&page=1\"\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url1)\n",
    "\n",
    "    \n",
    "    # Start Scraping\n",
    "    result = pd.DataFrame()\n",
    "    page = 1\n",
    "    start = 0\n",
    "    image_unique=0\n",
    "    check = \"false\"\n",
    "    while(image_unique<400 and check == \"false\"):\n",
    "        # Go to catalogue page\n",
    "        result = result.append(scrape_detail_page2(driver,category,collection,brand))\n",
    "        # Go to the next page\n",
    "        try:\n",
    "            result = result.drop_duplicates(['image'])\n",
    "            image_unique=len(result)\n",
    "            check_next = driver.find_element_by_css_selector('.ant-pagination-next').find_element_by_xpath('a')\n",
    "            check = driver.find_element_by_css_selector('.ant-pagination-next').get_attribute('aria-disabled')\n",
    "            check_next.click()\n",
    "            page = page+1\n",
    "            next_page = url + \"&page=\" + str(page)\n",
    "            print(\"Go to next page: \" + next_page)\n",
    "        except:\n",
    "            PrintException()\n",
    "            break\n",
    "    \n",
    "    driver.close()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pakaian_anak > dress\n",
      "Connected successfully!!!\n",
      "Go to next page: https://www.lazada.co.id/fashion-dress-anak-perempuan/?coolkids-official-store&from=wangpu&page=2\n",
      "                                   brand              category  \\\n",
      "0  /?coolkids-official-store&from=wangpu  pakaian_anak > dress   \n",
      "0  /?coolkids-official-store&from=wangpu  pakaian_anak > dress   \n",
      "0  /?coolkids-official-store&from=wangpu  pakaian_anak > dress   \n",
      "0  /?coolkids-official-store&from=wangpu  pakaian_anak > dress   \n",
      "0  /?coolkids-official-store&from=wangpu  pakaian_anak > dress   \n",
      "\n",
      "  discounted_price                                              image  \\\n",
      "0           359920  https://id-test-11.slatic.net/p/1366739c4f220f...   \n",
      "0           151920  https://id-live-01.slatic.net/original/64b4ee9...   \n",
      "0           359920  https://id-test-11.slatic.net/p/293b18df91aaaf...   \n",
      "0           143920  https://id-live-01.slatic.net/original/29cb823...   \n",
      "0           219900  https://id-live-01.slatic.net/original/efa062a...   \n",
      "\n",
      "                                                name origin_price  \\\n",
      "0                Coolkids - Denim Blue Gamis - 2243P       449900   \n",
      "0  Cool Girl - Dress Biru Lengan Pendek Full Prin...       189900   \n",
      "0                Coolkids - Denim Navy Gamis - 2223P       449900   \n",
      "0  Cool Baby - Blouse Pink Muda Lengan Pendek Ful...       179900   \n",
      "0  Cool Girl - Dress Pink Muda Lengan Pendek Full...       219900   \n",
      "\n",
      "  scraping_date        sku  \n",
      "0    2019-07-18  548604459  \n",
      "0    2019-07-18  407797660  \n",
      "0    2019-07-18  548608707  \n",
      "0    2019-07-18  407819609  \n",
      "0    2019-07-18  407824167  \n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "lazada_category = pd.read_csv('C:/Users/tamu/SharePoint/Product Development - Documents/Merchandising Analytics/Analysis - QBR Jul19/Data/scraping/lazada_category.csv',sep=',')\n",
    "result = pd.DataFrame()\n",
    "for x in range(1,len(result)):\n",
    "    category = lazada_category.iloc[x]\n",
    "    print(category['category'])\n",
    "    \n",
    "    brand = '/?coolkids-official-store&from=wangpu'\n",
    "    url = category['url']+brand\n",
    "    collection = category['collection']+\"_brand\"\n",
    "    \n",
    "    result = scrape_url3(url,category['category'],collection,brand)\n",
    "    print(result.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
